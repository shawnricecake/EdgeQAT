data:
  tokenizer_path: "/path_to/gpt-clean-16000.json"
    train_path: "/path_to/babylm_10M_clean"
    eval_path: "/path_to/babylm_dev_clean"
  seq_length: 128
  eval_samples: 8192

model:
  type: "GPT2" # or "Llama"
  name: "GPT2-97M"
  hidden_size: 768
  intermediate_size: None # train.py uses default = 4 * hidden_size 
  n_layer: 12
  n_head: 12 
  resid_pdrop: 0.0 # HF Llama doesn't have dropout
  attn_pdrop: 0.0
  embd_pdrop: 0.0

training:
  lr: 7e-4
  batch_size: 128
  num_epochs: 6
  gradient_accumulation_steps: 2
  warmup_steps: 300
  fp16: False

logging: 
  wandb: True
  project: "babylm-dev"
  output_dir: "/path_to/checkpoints/"

